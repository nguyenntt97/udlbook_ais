{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/udlbook/udlbook/blob/main/Notebooks/Chap03/3_4_Activation_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0F56yY8ohX"
      },
      "source": [
        "# **ノートブック 3.4 -- 活性化関数**\n",
        "\n",
        "この実習の目的は、異なる活性化関数を実験することです。<br>\n",
        "\n",
        "以下のセルを順番に実行してください。様々な場所で「TODO」という文字が表示されます。これらの場所では指示に従ってコードを書き、関数を完成させてください。テキストの中には質問も散りばめられています。\n",
        "\n",
        "間違いを見つけたり、提案がある場合は、udlbookmail@gmail.com までご連絡ください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GaDML3I8Yx4"
      },
      "outputs": [],
      "source": [
        "# 数学ライブラリをインポート\n",
        "import numpy as np\n",
        "# プロットライブラリをインポート\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeHzflFt9Tgn"
      },
      "outputs": [],
      "source": [
        "# 浅層ニューラルネットワークをプロット。入力は[0,1]の範囲、出力は[-1,1]の範囲と仮定\n",
        "# plot_allフラグがtrueに設定されている場合、図3.3のようにすべての中間段階をプロット\n",
        "def plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=False, x_data=None, y_data=None):\n",
        "\n",
        "  # フラグが設定されている場合、中間プロットを表示\n",
        "  if plot_all:\n",
        "    fig, ax = plt.subplots(3,3)\n",
        "    fig.set_size_inches(8.5, 8.5)\n",
        "    fig.tight_layout(pad=3.0)\n",
        "    ax[0,0].plot(x,pre_1,'r-'); ax[0,0].set_ylabel('事前活性化')\n",
        "    ax[0,1].plot(x,pre_2,'b-'); ax[0,1].set_ylabel('事前活性化')\n",
        "    ax[0,2].plot(x,pre_3,'g-'); ax[0,2].set_ylabel('事前活性化')\n",
        "    ax[1,0].plot(x,act_1,'r-'); ax[1,0].set_ylabel('活性化')\n",
        "    ax[1,1].plot(x,act_2,'b-'); ax[1,1].set_ylabel('活性化')\n",
        "    ax[1,2].plot(x,act_3,'g-'); ax[1,2].set_ylabel('活性化')\n",
        "    ax[2,0].plot(x,w_act_1,'r-'); ax[2,0].set_ylabel('重み付き活性化')\n",
        "    ax[2,1].plot(x,w_act_2,'b-'); ax[2,1].set_ylabel('重み付き活性化')\n",
        "    ax[2,2].plot(x,w_act_3,'g-'); ax[2,2].set_ylabel('重み付き活性化')\n",
        "\n",
        "    for plot_y in range(3):\n",
        "      for plot_x in range(3):\n",
        "        ax[plot_y,plot_x].set_xlim([0,1]);ax[plot_x,plot_y].set_ylim([-1,1])\n",
        "        ax[plot_y,plot_x].set_aspect(0.5)\n",
        "      ax[2,plot_y].set_xlabel('入力, $x$');\n",
        "    plt.show()\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(x,y)\n",
        "  ax.set_xlabel('入力, $x$'); ax.set_ylabel('出力, $y$')\n",
        "  ax.set_xlim([0,1]);ax.set_ylim([-1,1])\n",
        "  ax.set_aspect(0.5)\n",
        "  if x_data is not None:\n",
        "    ax.plot(x_data, y_data, 'mo')\n",
        "    for i in range(len(x_data)):\n",
        "      ax.plot(x_data[i], y_data[i],)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qeIUrh19AkH"
      },
      "outputs": [],
      "source": [
        "# 1つの入力、1つの出力、3つの隠れユニットを持つ浅層ニューラルネットワークを定義\n",
        "def shallow_1_1_3(x, activation_fn, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31):\n",
        "  pre_1 = theta_10 + theta_11 * x\n",
        "  pre_2 = theta_20 + theta_21 * x\n",
        "  pre_3 = theta_30 + theta_31 * x\n",
        "  # 図3.3 d-fのように、これらをReLU関数に通して活性化を計算\n",
        "  act_1 = activation_fn(pre_1)\n",
        "  act_2 = activation_fn(pre_2)\n",
        "  act_3 = activation_fn(pre_3)\n",
        "\n",
        "  w_act_1 = phi_1 * act_1\n",
        "  w_act_2 = phi_2 * act_2\n",
        "  w_act_3 = phi_3 * act_3\n",
        "\n",
        "  y = phi_0 + w_act_1 + w_act_2 + w_act_3\n",
        "\n",
        "  # 計算したすべてを返す\n",
        "  return y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwTp__Fk9YUx"
      },
      "outputs": [],
      "source": [
        "# 正規化線形ユニット（ReLU）関数を定義\n",
        "def ReLU(preactivation):\n",
        "  activation = preactivation.clip(0.0)\n",
        "  return activation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "INQkRzyn9kVC"
      },
      "source": [
        "まず、ReLU関数を使ってネットワークを実行してみましょう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT9QuKou9i0_"
      },
      "outputs": [],
      "source": [
        "# パラメータを定義してニューラルネットワークを実行\n",
        "theta_10 =  0.3 ; theta_11 = -1.0\n",
        "theta_20 = -1.0  ; theta_21 = 2.0\n",
        "theta_30 = -0.5  ; theta_31 = 0.65\n",
        "phi_0 = -0.3; phi_1 = 2.0; phi_2 = -1.0; phi_3 = 7.0\n",
        "\n",
        "# 入力値の範囲を定義\n",
        "x = np.arange(0,1,0.01)\n",
        "\n",
        "# これらの入力値のそれぞれに対してニューラルネットワークを実行\n",
        "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \\\n",
        "    shallow_1_1_3(x, ReLU, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
        "# そしてプロット\n",
        "plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-I8N7r1o9HYf"
      },
      "source": [
        "# シグモイド活性化関数\n",
        "\n",
        "ReLUは唯一の活性化関数ではありません。長い間、人々はシグモイド関数を使用していました。ロジスティックシグモイド関数は次の式で定義されます：\n",
        "\n",
        "\\begin{equation}\n",
        "f[z] = \\frac{1}{1+\\exp{[-10 z ]}}\n",
        "\\end{equation}\n",
        "\n",
        "（10の係数は標準的ではないことに注意してください -- しかし、これによりReLUの例と同じ軸でプロットできます）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgkioNyr975Y"
      },
      "outputs": [],
      "source": [
        "# シグモイド関数を定義\n",
        "def sigmoid(preactivation):\n",
        "  # TODO シグモイド関数を実装し、事前活性化から隠れユニットの活性化を計算するコードを書いてください。\n",
        "  # np.exp()関数を使用してください。\n",
        "  activation = np.zeros_like(preactivation);\n",
        "\n",
        "  return activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94HIXKJH97ve"
      },
      "outputs": [],
      "source": [
        "# 入力の配列を作成\n",
        "z = np.arange(-1,1,0.01)\n",
        "sig_z = sigmoid(z)\n",
        "\n",
        "# シグモイド関数をプロット\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(z,sig_z,'r-')\n",
        "ax.set_xlim([-1,1]);ax.set_ylim([0,1])\n",
        "ax.set_xlabel('z'); ax.set_ylabel('sig[z]')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "p3zQNXhj-J-o"
      },
      "source": [
        "ニューラルネットワークでこの活性化関数を使った場合に何が起こるか見てみましょう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1dASr9L-GNt"
      },
      "outputs": [],
      "source": [
        "theta_10 =  0.3 ; theta_11 = -1.0\n",
        "theta_20 = -1.0  ; theta_21 = 2.0\n",
        "theta_30 = -0.5  ; theta_31 = 0.65\n",
        "phi_0 = 0.3; phi_1 = 0.5; phi_2 = -1.0; phi_3 = 0.9\n",
        "\n",
        "# 入力値の範囲を定義\n",
        "x = np.arange(0,1,0.01)\n",
        "\n",
        "# これらの入力値のそれぞれに対してニューラルネットワークを実行\n",
        "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \\\n",
        "    shallow_1_1_3(x, sigmoid, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
        "# そしてプロット\n",
        "plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Uuam_DewA9fH"
      },
      "source": [
        "おそらく、これが滑らかな曲線を与えることに気づくでしょう。では、なぜこれを使わないのでしょうか？ああ...今は明らかではありませんが、モデルの適合を学ぶときにその理由に辿り着くでしょう。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C9WKkcMUABze"
      },
      "source": [
        "# ヘビサイド活性化関数\n",
        "\n",
        "ヘビサイド関数は次のように定義されます：\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{heaviside}[z] = \\begin{cases} 0 & \\quad z <0 \\\\ 1 & \\quad z\\geq 0\\end{cases}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1qFkdOL-NPc"
      },
      "outputs": [],
      "source": [
        "# ヘビサイド関数を定義\n",
        "def heaviside(preactivation):\n",
        "  # TODO ヘビサイド関数を実装し、事前活性化から隠れユニットの活性化を計算するコードを書いてください。\n",
        "  # 実装によっては、ブール配列を0と1の配列に変換する必要があります。これを行うには、.astype(int)を使用してください。\n",
        "  activation = np.zeros_like(preactivation);\n",
        "\n",
        "\n",
        "  return activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSPyp7iA-44H"
      },
      "outputs": [],
      "source": [
        "# 入力の配列を作成\n",
        "z = np.arange(-1,1,0.01)\n",
        "heav_z = heaviside(z)\n",
        "\n",
        "# ヘビサイド関数をプロット\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(z,heav_z,'r-')\n",
        "ax.set_xlim([-1,1]);ax.set_ylim([-2,2])\n",
        "ax.set_xlabel('z'); ax.set_ylabel('heaviside[z]')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t99K2lSl--Mq"
      },
      "outputs": [],
      "source": [
        "theta_10 =  0.3 ; theta_11 = -1.0\n",
        "theta_20 = -1.0  ; theta_21 = 2.0\n",
        "theta_30 = -0.5  ; theta_31 = 0.65\n",
        "phi_0 = 0.3; phi_1 = 0.5; phi_2 = -1.0; phi_3 = 0.9\n",
        "\n",
        "# 入力値の範囲を定義\n",
        "x = np.arange(0,1,0.01)\n",
        "\n",
        "# これらの入力値のそれぞれに対してニューラルネットワークを実行\n",
        "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \\\n",
        "    shallow_1_1_3(x, heaviside, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
        "# そしてプロット\n",
        "plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T65MRtM-BCQA"
      },
      "source": [
        "これはあらゆる関数を近似できますが、出力は不連続であり、モデル適合についてさらに学ぶときに発見する理由により、使用しない理由もあります。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RkB-XZMLBTaR"
      },
      "source": [
        "# 線形活性化関数\n",
        "\n",
        "活性化関数が線形の場合、ニューラルネットワークは機能しません。例えば、活性化関数が次のような場合に何が起こるかを考えてみてください：\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{lin}[z] = a + bz\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q59v3saj_jq1"
      },
      "outputs": [],
      "source": [
        "# 線形活性化関数を定義\n",
        "def lin(preactivation):\n",
        "  a =0\n",
        "  b =1\n",
        "  # 線形関数を計算\n",
        "  activation = a+b * preactivation\n",
        "  # 返す\n",
        "  return activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwodsBr0BkDn"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# 1. 上記の線形活性化関数は入力をそのまま返します: (0+1*z) = z\n",
        "# コードを実行する前に、描画の10のパネルがどのように見えるか予測してみてください\n",
        "# 今度は以下のコードを実行して、予測が正しかったかどうか確認してください。これはどのような関数族を表現できますか？\n",
        "\n",
        "# 2. パラメータ(a,b)を異なる値に変更すると何が起こりますか？\n",
        "# a=0.5, b=-0.4を試してみてください。関数を更新するためにセルを再実行することを忘れないでください\n",
        "\n",
        "theta_10 =  0.3 ; theta_11 = -1.0\n",
        "theta_20 = -1.0  ; theta_21 = 2.0\n",
        "theta_30 = -0.5  ; theta_31 = 0.65\n",
        "phi_0 = 0.3; phi_1 = 0.5; phi_2 = -1.0; phi_3 = 0.9\n",
        "\n",
        "# 入力値の範囲を定義\n",
        "x = np.arange(0,1,0.01)\n",
        "\n",
        "# これらの入力値のそれぞれに対してニューラルネットワークを実行\n",
        "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \\\n",
        "    shallow_1_1_3(x, lin, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
        "# そしてプロット\n",
        "plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOmxhh3ymYWX+1HdZ91I6zU",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
